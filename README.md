

# <img src="assets/logo_mexa.png" alt="Image description" class="title-icon" style="width: 40px; height: auto;"> MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation
 [![arXiv](https://img.shields.io/badge/arXiv-2402.05889-b31b1b.svg)](https://arxiv.org/abs/2402.05889)


### Authors: [Shoubin Yu*](https://yui010206.github.io/),[Yue Zhang*](https://zhangyuejoslin.github.io/),[Ziyang Wang](https://ziyangw2000.github.io/),[Jaehong Yoon](https://jaehong31.github.io/), [Mohit Bansal](https://www.cs.unc.edu/~mbansal/)
### University of North Carolina at Chapel Hill


<br>
<img src="./assets/method.png" alt="teaser image" width="1000"/>

# ðŸ”¥ News
- **Jun 22, 2025**. Check our [arXiv-version]() for MEXA.



# Setup

- We will release skill caption code / data later

## Install Dependencies


```bash
conda create -n mexa python=3.10
conda activate mexa
pip install -r requirements.txt
```


# Inference
We provide MEXA inference script examples as follows.

```bash
sh run_mexa.sh
```


# Reference
Please cite our paper if you use our models in your works:

```bibtex
@article{yu2025mexa,
  title={MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation},
  author={Yu, Shoubin and Zhang, Yue and Wang, Ziyang and Yoon, Jaehong and Bansal, Mohit},
  journal={ArXiv},
  year={2025}
}
